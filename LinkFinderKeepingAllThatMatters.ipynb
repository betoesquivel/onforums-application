{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from testdataextractor.testdataextractor.extractor import Extractor\n",
    "from summpy.summpy import lexrank\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "testarticles = [1957284403,1965754064,233465322,283147769,362778020,37793736,389321649,540607195,60134403,887344770, ]\n",
    "all_articles = []\n",
    "all_sets_sentences = []\n",
    "all_article_sentences = []\n",
    "all_comments_sentences = []\n",
    "all_groups = [] # each comment is a group, the article counts as a comment\n",
    "for art in testarticles:\n",
    "    ext = Extractor(\"../test_data/{0}.ofs.gold.xml\".format(art))\n",
    "    article = ext.extract(verbose=True)\n",
    "    all_articles.append(article)\n",
    "    \n",
    "    df_article = pd.DataFrame.from_dict(article['sentences'], orient='index')\n",
    "    sorted_indexes = [ \"s{0}\".format(x) for x in range(len(article['sentences'].values()))]\n",
    "    sentences = list(df_article.ix[sorted_indexes, 'text'])\n",
    "    \n",
    "    ordered_sentences = df_article.ix[sorted_indexes]\n",
    "    article_sentences_ix = ordered_sentences.ix[:,'comment'].isnull()\n",
    "    \n",
    "    art_sentences = ordered_sentences[article_sentences_ix]\n",
    "    com_sentences = ordered_sentences[article_sentences_ix == False]\n",
    "    \n",
    "    article_sentences = list(art_sentences['text'])\n",
    "    comment_sentences = list(com_sentences['text'])\n",
    "    \n",
    "    groups_of_sentences = ordered_sentences.groupby(by='comment', sort=False)\n",
    "    groupcount = len(groups_of_sentences.groups.keys())\n",
    "    index = [\"c{0}\".format(i) for i in range(groupcount)]\n",
    "    grouped_comments = []\n",
    "    for g in index:\n",
    "        com = \" | \".join(list(groups_of_sentences.get_group(g)['text']))\n",
    "        grouped_comments.append(com)\n",
    "\n",
    "    grouped_comments.append(\" | \".join(article_sentences))\n",
    "    \n",
    "    all_article_sentences.append(article_sentences)\n",
    "    all_comments_sentences.append(comment_sentences)\n",
    "    all_groups.append(grouped_comments)\n",
    "    \n",
    "    if df_article.ix['s2', 'text'] == sentences[2]:\n",
    "        print \"Extracted list of sentences is in a proper order.\"\n",
    "        all_sets_sentences.append(sentences)\n",
    "    else:\n",
    "        print \"Extracted list of sentences is unordered.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence-sentence pairs with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk.stem\n",
    "import math\n",
    "\n",
    "def preprocess_docs(documents):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    texts = [tokenizer.tokenize(d) for d in documents]\n",
    "\n",
    "    stemmed_texts = []\n",
    "    for text in texts:\n",
    "        stemmed_text = [english_stemmer.stem(t) for t in text]\n",
    "        stemmed_texts.append(stemmed_text)\n",
    "    return stemmed_texts\n",
    "\n",
    "def strong_similarities_and_appropriate_links_thresh(lsi_queries, index):\n",
    "    '''\n",
    "    Returns a similarity dictionary with all the sentences\n",
    "    in lsi_queries, and their lists of strongest links tuples\n",
    "    with the sentence id link and the similarity percentage.\n",
    "    '''\n",
    "    total_links = 0\n",
    "    similarity_dict = {}\n",
    "\n",
    "    for i, query in enumerate(lsi_queries):\n",
    "        sims = index[query]\n",
    "        \n",
    "        strong_sims = [s for s in list(enumerate(sims)) if s[1] > 0.999]\n",
    "\n",
    "        similarity_dict[i] = strong_sims\n",
    "        links = len(strong_sims)\n",
    "        \n",
    "        total_links += links\n",
    "\n",
    "    # max_links is the average number of links per query sentence\n",
    "    min_links = 1\n",
    "    max_links = math.ceil(total_links/float(len(lsi_queries))) \n",
    "    thresh = (min_links, max_links) # non-inclusive\n",
    "    return similarity_dict, thresh\n",
    "\n",
    "\n",
    "def perform_queries_and_get_links(lsi_queries, index):\n",
    "    s_dict, thresh = strong_similarities_and_appropriate_links_thresh(lsi_queries,\n",
    "                                                                      index)\n",
    "    pruned_dict = {sid: simils for sid, simils in zip(s_dict.keys(), s_dict.values()) \n",
    "                   if len(simils) > thresh[0] and len(simils) < thresh[1]}\n",
    "    \n",
    "    strong_sentences = len(pruned_dict.keys())\n",
    "    selected_pairs = sum([len(x) for x in pruned_dict.values()])\n",
    "    \n",
    "    print \"\\n{0} strong sentences\".format(strong_sentences)\n",
    "    print \"{0} total sentence-sentence pairs\".format(selected_pairs)\n",
    "    print thresh\n",
    "    return pruned_dict\n",
    "\n",
    "def find_links_between_in(documents, comments_sentences):    \n",
    "    stemmed_texts = preprocess_docs(documents)\n",
    "    dictionary = corpora.Dictionary(stemmed_texts)\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "    corpus = [dictionary.doc2bow(text) for text in stemmed_texts]\n",
    "\n",
    "    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "\n",
    "#     comment_start_index = len(article_sentences)\n",
    "    stemmed_queries = preprocess_docs(comments_sentences)\n",
    "    query_dict = corpora.Dictionary(stemmed_queries)\n",
    "    lsi_queries = [lsi[query_dict.doc2bow(text)] for text in stemmed_queries]\n",
    "    similarity_dict = perform_queries_and_get_links(lsi_queries, index)\n",
    "    return similarity_dict\n",
    "\n",
    "documents = all_groups[0]\n",
    "comments_s = all_comments_sentences[0]\n",
    "article_s = all_article_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_sentence_links_in_all_articles():\n",
    "    all_similarity_dicts = []\n",
    "    for i, docs in enumerate(all_sets_sentences):\n",
    "        comments = all_comments_sentences[i]\n",
    "        article = all_article_sentences[i]\n",
    "        print \"\\n\\nARTICLE {0}\".format(i)\n",
    "        s_dict = find_links_between_in(docs, comments)\n",
    "        all_similarity_dicts.append(s_dict)\n",
    "    return all_similarity_dicts\n",
    "\n",
    "% time all_similarity_dicts = find_sentence_links_in_all_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def output_top_sentence_pairs(s_dict, all_art_sentences,\n",
    "                              all_sentences, \n",
    "                              all_comment_sentences):\n",
    "    d = s_dict\n",
    "    comment_start_index = len(all_art_sentences)\n",
    "    sentences = all_sentences\n",
    "    comment_sentences = all_comment_sentences\n",
    "    \n",
    "    for comment_sentence, links in zip(d.keys(),d.values()):\n",
    "\n",
    "        s1_id = \"s{0}\".format(comment_sentence + comment_start_index)\n",
    "        s2_id = \"s{0}\".format(links[0][0])\n",
    "        print \"\\nLink found\"\n",
    "        print comment_sentence+comment_start_index, [l[0] for l in links]        \n",
    "        print \"s{0} is:\\n{1}\\nSimilar too:\".format(comment_sentence+comment_start_index,\n",
    "                                                  comment_sentences[comment_sentence].encode('utf8'))\n",
    "        for i, (l_id, prob) in enumerate(links):\n",
    "            print \"S{0}: {1}\".format(l_id,sentences[l_id].encode('utf8'))\n",
    "\n",
    "for i, _ in enumerate(all_articles):\n",
    "    print \"\\nARTICLE {0}==========\".format(i)\n",
    "    output_top_sentence_pairs(all_similarity_dicts[i],\n",
    "                              all_article_sentences[i],\n",
    "                              all_sets_sentences[i],\n",
    "                              all_comments_sentences[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
