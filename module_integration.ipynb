{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess, CrawlerRunner\n",
    "from scrapy.settings import Settings\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.utils.log import configure_logging\n",
    "from scraper.guardianukscraper.spiders.guardian_spider import GuardianSpider\n",
    "from scraper.guardianukscraper import settings\n",
    "import os\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "os.environ['SCRAPY_SETTINGS_MODULE'] = 'scraper.guardianukscraper.settings'\n",
    "settings_module_path = os.environ['SCRAPY_SETTINGS_MODULE']\n",
    "settings = Settings()\n",
    "settings.setmodule(settings_module_path, priority='project')\n",
    "\n",
    "print settings['MONGODB_SERVER']\n",
    "configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n",
    "\n",
    "crawler = scrapy.crawler.Crawler(GuardianSpider,settings)\n",
    "\n",
    "process.crawl(crawler)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-42909157d6b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mart1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'links'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m classify_links(summary['links'], \n",
      "\u001b[0;32m<ipython-input-2-42909157d6b8>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(article_dict)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mpreprocess_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-42909157d6b8>\u001b[0m in \u001b[0;36mpreprocess_article\u001b[0;34m(article_dict)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0morder_comments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0msplit_into_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_start_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-42909157d6b8>\u001b[0m in \u001b[0;36msplit_into_sentences\u001b[0;34m(article_dict)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     a_dicts, a_sents = article_body_into_sentences_dict_and_list(\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0marticle_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m     \u001b[0mstart_comment_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'body'"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from scraper.guardianukscraper import settings\n",
    "from scrapy.settings import Settings\n",
    "import logging\n",
    "\n",
    "from linkfinder.linkfinder.link_finder import preprocess_docs\n",
    "from linkfinder.linkfinder.link_finder import strong_similarities_and_appropriate_links_thresh\n",
    "from linkfinder.linkfinder.link_finder import perform_queries_and_get_links\n",
    "from linkfinder.linkfinder.link_finder import find_links_between_in\n",
    "\n",
    "sets = Settings()\n",
    "sets.setmodule(settings, priority='project')\n",
    "connection = pymongo.MongoClient(\n",
    "        sets['MONGODB_SERVER'],\n",
    "        sets['MONGODB_PORT']\n",
    ")\n",
    "db = connection[sets['MONGODB_DB']]\n",
    "collection = db[sets['MONGODB_COLLECTION']]\n",
    "\n",
    "def order_comments(comments):\n",
    "    comments.reverse()\n",
    "\n",
    "def join_all_comments_paragraphs(comments):\n",
    "    for comment in comments:\n",
    "        if comment['content'].__class__ == list:\n",
    "            comment['content'] = \" \\n \".join(comment['content'])\n",
    "            \n",
    "def text_list_into_sentences_dict_and_list(text, start_id):\n",
    "    import nltk.data\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    dicts = []\n",
    "    sentences = []\n",
    "    \n",
    "    if text.__class__ == list:\n",
    "        for i, t in enumerate(text):\n",
    "            for s in sent_detector.tokenize(t):\n",
    "                new = {\n",
    "                    'text': s,\n",
    "                    'comment': i\n",
    "                }\n",
    "                dicts.append(new)\n",
    "                sentences.append(s)\n",
    "    else:\n",
    "        for s in sent_detector.tokenize(text):\n",
    "                new = {\n",
    "                    'text': s,\n",
    "                    'comment': -1\n",
    "                }\n",
    "                dicts.append(new)\n",
    "                sentences.append(s)\n",
    "    return dicts, sentences\n",
    "\n",
    "def article_body_into_sentences_dict_and_list(article_body):\n",
    "    joined_body = \" \\n \".join(article_body)\n",
    "    dicts, sentences = text_list_into_sentences_dict_and_list(joined_body,\n",
    "                                                             0)\n",
    "    return dicts, sentences\n",
    "\n",
    "def comments_to_sentences_dict_and_list(comments, start_id):\n",
    "    join_all_comments_paragraphs(comments)\n",
    "    paragraphs = [c['content'] for c in comments]\n",
    "    dicts, sentences = text_list_into_sentences_dict_and_list(paragraphs,\n",
    "                                                             start_id)\n",
    "    return dicts, sentences\n",
    "\n",
    "def split_into_sentences(article_dict):\n",
    "    import copy\n",
    "    \n",
    "    a_dicts, a_sents = article_body_into_sentences_dict_and_list(\n",
    "        article_dict['body']\n",
    "    )\n",
    "    start_comment_id = len(a_sents)\n",
    "    c_dicts, c_sents = comments_to_sentences_dict_and_list(\n",
    "        article_dict['comments'],\n",
    "        start_comment_id\n",
    "    )\n",
    "#     if a_dicts is None or c_dicts is None:\n",
    "#         print \"Dicts are empty\"\n",
    "#     else:\n",
    "# #         print \"THESE ARE THE DICTS:\\n{0}\".format(c_dicts)\n",
    "#         print c_dicts[0]\n",
    "#         print article_dict['comments'][0]['content']\n",
    "    \n",
    "    all_dicts = copy.deepcopy(a_dicts)\n",
    "    all_sentences = copy.deepcopy(a_sents)\n",
    "    all_dicts.extend(copy.deepcopy(c_dicts))\n",
    "    all_sentences.extend(copy.deepcopy(c_sents))\n",
    "    \n",
    "    article_dict['comment_sentences'] = c_sents\n",
    "    article_dict['article_sentences'] = a_sents\n",
    "    article_dict['all_sentences'] = all_sentences\n",
    "    article_dict['all_sentences_dicts'] = all_dicts\n",
    "    \n",
    "#     if all_dicts[len(article_dict['article_sentences'])]['text'] in article_dict['comments'][c_dicts[0]['comment']]['content']:\n",
    "#         print \"Comment sentences keep a reference to their comment. Passed.\"\n",
    "    \n",
    "def preprocess_article(article_dict):\n",
    "    order_comments(article_dict['comments'])\n",
    "    split_into_sentences(article_dict)\n",
    "\n",
    "def classify_links(s_dict, all_sentences, comment_start_index):\n",
    "    '''\n",
    "    Receive a similarity links structure in the form of \n",
    "    {\n",
    "    comment_sentence_no: [(list of tuples with sentence id,\n",
    "                        and percentage)]\n",
    "    }\n",
    "    \n",
    "    comment_start_index has the offset to add to the comment\n",
    "    sentence number to change it into an id.\n",
    "    \n",
    "    It changes the list of links into a triple, with the:\n",
    "    (sentence_id, percentage, \"type of link\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    for comment_no, link_list in s_dict.iteritems():\n",
    "        comment_sentence_id = comment_no + comment_start_index\n",
    "        classified_links = []\n",
    "        for l in link_list:\n",
    "            # category = classify(all_sentences[comment_sentence_id],\n",
    "            #           all_sentences[l[0]])\n",
    "            category = 'stub'\n",
    "            classified_links.append((l[0], l[1].item(), category))\n",
    "        s_dict[comment_no] = classified_links\n",
    "    \n",
    "def summarize(article_dict):\n",
    "    preprocess_article(article_dict)\n",
    "    summary = copy.deepcopy(article_dict)\n",
    "    del summary['body']\n",
    "    \n",
    "    docs = summary['all_sentences']\n",
    "    comments = summary['comment_sentences']\n",
    "    \n",
    "    # not that the dictionary doesn't have the sentence id, but the\n",
    "    # comment sentence number as key. So you have to add\n",
    "    # len(summary['article_sentences']) to the key, to get the \n",
    "    # comment sentence id.\n",
    "    similarity_dict = find_links_between_in(docs, comments)\n",
    "    summary['links'] = similarity_dict\n",
    "    print summary.keys()\n",
    "    return summary\n",
    "    \n",
    "art1 = collection.find_one()\n",
    "summary = summarize(art1)\n",
    "print summary['links'][0]\n",
    "classify_links(summary['links'], \n",
    "               summary['all_sentences'], \n",
    "               len(summary['article_sentences']))\n",
    "def convert_keys_to_string(dictionary):\n",
    "    \"\"\"Recursively converts dictionary keys to strings.\"\"\"\n",
    "    if not isinstance(dictionary, dict):\n",
    "        return dictionary\n",
    "    return dict((str(k), convert_keys_to_string(v)) \n",
    "        for k, v in dictionary.items())\n",
    "\n",
    "str_summary = convert_keys_to_string(summary)\n",
    "\n",
    "str_summary['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_top_sentence_pairs(s_dict, all_art_sentences,\n",
    "                              all_sentences, \n",
    "                              all_comment_sentences):\n",
    "    d = s_dict\n",
    "    comment_start_index = len(all_art_sentences)\n",
    "    sentences = all_sentences\n",
    "    comment_sentences = all_comment_sentences\n",
    "    \n",
    "    for comment_sentence, links in zip(d.keys(),d.values()):\n",
    "\n",
    "        s1_id = \"s{0}\".format(comment_sentence + comment_start_index)\n",
    "        s2_id = \"s{0}\".format(links[0][0])\n",
    "        print \"\\nLink found\"\n",
    "        print comment_sentence+comment_start_index, [l[0] for l in links]        \n",
    "        print \"s{0} is:\\n{1}\\nSimilar too:\".format(comment_sentence+comment_start_index,\n",
    "                                                  comment_sentences[comment_sentence].strip().encode('utf8'))\n",
    "        for i, (l_id, prob) in enumerate(links):\n",
    "            print \"S{0}: {1}\".format(l_id,sentences[l_id].strip().encode('utf8'))\n",
    "\n",
    "% time\n",
    "output_top_sentence_pairs(summary['links'],\n",
    "                         summary['article_sentences'],\n",
    "                         summary['all_sentences'],\n",
    "                         summary['comment_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8460f9057e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summaries'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msumm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msumm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "collection = db['summaries']\n",
    "summ = collection.find_one()\n",
    "summ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
